{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Apache Spark for Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Easy to use. Provides high-level API that focuses on the content of the computation.\n",
    "2. Fast, enabling interactive use and complex algorithms.\n",
    "3. General engine. Combines multiple types of computations (SQL queries, text processing, and ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Introduction to Data Analysis with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Apache Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apache Spark is a cluster computing platform designed to be fast and general-purpose.\n",
    "2. Ability to run computation in memory.\n",
    "3. More efficient than MapReduce for complex applications.\n",
    "4. Integrate closely with other Big Data tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Unified Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark Core - Task scheduling, memory management, RDD API\n",
    "2. Spark SQL - Structured data\n",
    "3. Spark streaming - Live stream of data in real time\n",
    "4. MLlib machine learning\n",
    "5. GraphX graph processing\n",
    "6. Cluster Managers - Standalone, YARN, Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Scientist\n",
    "2. Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Downloading Spark and Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark shell allow us to interact with data that is distributed on disk or in memory across many machines.\n",
    "Provides Scala and Python shells.\n",
    "\n",
    "1. Scala shell: bin/spark-shell\n",
    "2. Python shell (PySpark): bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing verbosity of logging in spark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of conf/log4j.properties.template called conf/log4j.properties and find the following line:  \n",
    "log4j.rootCategory=INFO, console  \n",
    "And change it to  \n",
    "log4j.rootCategory=WARN, console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('file:///usr/local/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///usr/local/spark/README.md MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'# Apache Spark'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Core Spark Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Spark application consists of a driver program that launches various parallel operations on a cluster.  \n",
    "Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.  \n",
    "Driver programs manages a number of nodes called executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standalone applications, such as scripts, we have to initialize our own SparkContext.  \n",
    "In Java and Scala, one has to give the application a Maven dependency on the spark-core artifact.  \n",
    "In Python, application must be run using bin/spark-submit script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('My App')\n",
    "#sc = SparkContext(conf=conf) # Spark context already running inside Ipython notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
