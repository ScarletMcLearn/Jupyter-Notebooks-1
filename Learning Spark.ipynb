{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Apache Spark for Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Easy to use. Provides high-level API that focuses on the content of the computation.\n",
    "2. Fast, enabling interactive use and complex algorithms.\n",
    "3. General engine. Combines multiple types of computations (SQL queries, text processing, and ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Data Analysis with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apache Spark is a cluster computing platform designed to be fast and general-purpose.\n",
    "2. Ability to run computation in memory.\n",
    "3. More efficient than MapReduce for complex applications.\n",
    "4. Integrate closely with other Big Data tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Unified Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark Core - Task scheduling, memory management, RDD API\n",
    "2. Spark SQL - Structured data\n",
    "3. Spark streaming - Live stream of data in real time\n",
    "4. MLlib machine learning\n",
    "5. GraphX graph processing\n",
    "6. Cluster Managers - Standalone, YARN, Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Scientist\n",
    "2. Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Downloading Spark and Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark shell allow us to interact with data that is distributed on disk or in memory across many machines.\n",
    "Provides Scala and Python shells.\n",
    "\n",
    "1. Scala shell: bin/spark-shell\n",
    "2. Python shell (PySpark): bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing verbosity of logging in spark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of conf/log4j.properties.template called conf/log4j.properties and find the following line:  \n",
    "log4j.rootCategory=INFO, console  \n",
    "And change it to  \n",
    "log4j.rootCategory=WARN, console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('file:///usr/local/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///usr/local/spark/README.md MapPartitionsRDD[4] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'# Apache Spark'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Core Spark Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Spark application consists of a driver program that launches various parallel operations on a cluster.  \n",
    "Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.  \n",
    "Driver programs manages a number of nodes called executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standalone applications, such as scripts, we have to initialize our own SparkContext.  \n",
    "In Java and Scala, one has to give the application a Maven dependency on the spark-core artifact.  \n",
    "In Python, application must be run using bin/spark-submit script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('My App')\n",
    "#sc = SparkContext(conf=conf) # Spark context already running inside Ipython notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Programming with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient Distributed Dataset (RDD) is distributed collection of elements.  \n",
    "In Spark, all works is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.  \n",
    "RDDs can contain any type of Python, Java, or Scala objects, including user defined classes.  \n",
    "\n",
    "Creating RDD:\n",
    "\n",
    "1. Loading an external dataset\n",
    "2. Distributing a collection of objects in driver program.\n",
    "\n",
    "Once created, RDDs offer two types of operations.\n",
    "\n",
    "1. Transformations - construct a new RDD from a previous one.\n",
    "2. Actions - compute a result based on an RDD, and either return it to the driver program or save it to an external sotrage system. (e.g HDFS)\n",
    "\n",
    "Spark performs transformations in lazy fashion, i.e transformations are only computed when an action is called.\n",
    "\n",
    "Spark RDDs are by default recomputed each time one run an action on them. In order to overcome this, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Spark application will work as follows:\n",
    "\n",
    "1. Create RDD\n",
    "2. Transform RDD\n",
    "3. Persist RDD\n",
    "4. Perfom Action on RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take an existing collection in your program and pass it to SparkContext's parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[7] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize(['pandas', 'I like pandas'])\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load data from external storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///usr/local/spark/README.md MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('file:///usr/local/spark/README.md')\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transformations - returns RDD\n",
    "2. Actions - returns data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed RDDs are computed lazily, only when one use them in an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile('log.txt')\n",
    "errorsRDD = inputRDD.filter(lambda x: 'error' in x)\n",
    "warningsRDD = inputRDD.filter(lambda x: 'warning' in x)\n",
    "# badLinesRDD = errorsRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.  \n",
    "It uses this information to compute each RDD on demand and to recover lost data if part of persistent RDD is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations that return a final value to the driver program or write data to an external storage system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 'Input had ' + badLinesRDD.count() + ' concerning lines'\n",
    "# print 'Here are the 10 examples'\n",
    "# for line in badLinesRDD.take(10):\n",
    "#     print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD also have collect() function to retrieve the entire RDD.  \n",
    "In order to collect large RDD, better save the content of an RDD using saveAsTextFile() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call a transformation on an RDD, the operation is not immediately performed.  \n",
    "Think of each RDD as consisting of instructions on how to compute the data that we build up through transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing functions to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of Spark’s transformations, and some of its actions, depend on passing in functions that are used by Spark to compute data.\n",
    "\n",
    "Three options for passing functions\n",
    "1. lambda\n",
    "2. Top-level functions\n",
    "3. Locally defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. map() - takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD. \n",
    "2. filter() - takes in a function and returns an RDD that only has elements that pass the filter() function.\n",
    "3. flatMap() - we return an iterator with our return values. Rather than producing an RDD of iterators, we get back an RDD that consists of the elements from all of the iterators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['hello world', 'hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'], ['hi']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.map(lambda line: line.split())\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'hi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda line: line.split())\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo set operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. rdd.distinct()\n",
    "2. rdd1.union(rdd2)\n",
    "3. rdd1.intersection(rdd2)\n",
    "4. rdd1.subtract(rdd2)\n",
    "5. rdd1.cartesian(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reduce() - takes a function that operates on two elements of the type in your RDD and returns a new element of the same type.\n",
    "2. fold() - takes a function with the same signature as needed for reduce(), but in addition takes a “zero value” to be used for the initial call on each partition. The zero value you provide should be the identity element for your operation.\n",
    "3. aggregate() - we supply an initial zero value of the type we want to return. We then supply a function to combine the elements from our RDD with the accumulator. Finally, we need to supply a second function to merge two accumulators, given that each node accumulates its own results locally.\n",
    "4. collect() - returns the entire RDD's content to the driver.\n",
    "5. take(n) - returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection.\n",
    "6. top() - extract the top elements from an RDD.\n",
    "7. takeSample(withReplacement, num, seed) - take a sample of our data either with or without replacement.\n",
    "8. foreach() - lets us perform computations on each element in the RDD without bringing it back locally.\n",
    "9. count()\n",
    "10. countByValue() - returns a map of each unique value to its count.\n",
    "\n",
    "Note: Return type of the result in reduce() and fold() should be the same type as that of the elements in the RDD we are operating over.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting between RDD types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions are available only on certain types of RDDs, such as mean() and variance() on numeric RDDs or join() on key/value pair RDDs.  \n",
    "In Scala and Java, these methods aren’t defined on the standard RDD class, so to access this additional functionality we have to make sure we get the correct specialized class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence (Caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid computing an RDD multiple times, we can ask Spark to persist the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Working with Key/Value Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides special operations on RDDs containing key/value pairs. These RDDs are called pair RDDs. Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Few reading formats directly return pair RDDs for their key/value data.\n",
    "2. Use map() to convert RDD into Pair RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 1), ('hi', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = words.map(lambda w: (w, 1))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reduceByKey()\n",
    "2. groupByKey()\n",
    "3. combineByKey()\n",
    "4. mapValues()\n",
    "5. flatMapValues()\n",
    "6. keys()\n",
    "7. values()\n",
    "8. sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reduceByKey() - runs several parallel reduce operations, one for each key in the dataset, where each operation combines values that have the same key.\n",
    "2. foldByKey()\n",
    "3. combineByKey() - is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 4),\n",
       " (u'computation', 1),\n",
       " (u'contributing', 1),\n",
       " (u'submit', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'Scala,', 1),\n",
       " (u'environment', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word Count\n",
    "rdd = sc.textFile('file:///usr/local/spark/README.md')\n",
    "words = rdd.flatMap(lambda x: x.split())\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the level of parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will always try to infer a sensible default value based on the size of your cluster, but in some cases you will want to tune the level of parallelism for better performance.  \n",
    "Pass number of paritions or use repartition() or coalesce() for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. groupByKey() - If our data is already keyed in the way we want, groupByKey() will group our data using the key in our RDD. On an RDD consisting of keys of type K and values of type V, we get back an RDD of type [K, Iterable[V]].\n",
    "2. groupBy() - works on unpaired data or data where we want to use a different condition besides equality on the current key. It takes a function that it applies to every element in the source RDD and uses the result to determine the key.\n",
    "3. cogroup() - group data sharing the same key from multiple RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. rdd1.join(rdd2)\n",
    "2. rdd1.leftOuterJoin(rdd2)\n",
    "3. rdd1.rightOuterJoin(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions Available on the Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. countByKey()\n",
    "2. collectAsMap()\n",
    "3. lookup(key) - Return all values associated with the provided key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark programs can choose to control their RDDs’ partitioning to reduce communication.  \n",
    "Use partitionBy() transformation at the start of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining an RDD's partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use rdd.partitioner in Scala and Java to determine the partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations that benefit from Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cogroup(), groupWith(), join(), leftOuterJoin(), rightOuter Join(), groupByKey(), reduceByKey(), combineByKey(), and lookup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Partitioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Spark’s HashPartitioner and RangePartitioner are well suited to many use cases, Spark also allows you to tune how an RDD is partitioned by providing a custom Partitioner object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Loading and Saving your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three common sets of data sources:\n",
    "\n",
    "1. File formats and filesystems - local or distributed filesystem.\n",
    "2. Structured data sources through Spark SQL\n",
    "3. Databases and key/value stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading:  \n",
    "sc.texfile() - load a single text file as an RDD, each input line becomes an element in the RDD.  \n",
    "sc.wholeTextFiles() - load multiple whole text files at the same time into a pair RDD, with the key being the name and the value being the contents of each file.  \n",
    "\n",
    "Saving:  \n",
    "result.saveAsTextFile() - The path is treated as a directory and Spark will output multiple files underneath that directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data as a text file and then parsing the JSON data is an approach that we can use in all of the supported languages. This works assuming that you have one JSON record per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = rdd.map(lambda x: json.loads(x))\n",
    "\n",
    "# data.filter(lambda x: x['lovesPandas']).map(lambda x: json.dumps(x)).saveAsTextFile(outputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV and TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CSV/TSV data is similar to loading JSON data in that we can first load it as text and then process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceFiles are a popular Hadoop format composed of flat files with key/value pairs. SequenceFiles have sync markers that allow Spark to seek to a point in the file and then resynchronize with the record boundaries. This allows Spark to efficiently read SequenceFiles in parallel from multiple nodes.\n",
    "\n",
    "Use sc.sequenceFile() function to read sequence files  \n",
    "Use pairRDD.saveAsSequenceFile() to save sequence file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sc.objectFile() to read an object file  \n",
    "Use rdd.saveAsObjectFile() to save an object file\n",
    "\n",
    "In Python, use saveAsPickleFile() and pickleFile() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Input/Output formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sc.hadoopFile() to load old Hadoop file  \n",
    "Use sc.newAPIHadoopFile() to load new Hadoop file  \n",
    "Use rdd.saveAsHadoopFile() to save an RDD as an old Hadoop file  \n",
    "Use rdd.saveAsNewAPIHadoopFile() to save an RDD as a new Hadoop file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-filesystem data sources\n",
    "1. Protocol buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Big Data, we find ourselves needing to use compressed data to save storage space and network overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports a large number of filesystems for reading and writing to, which we can use with any of the file formats we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local/Regular FS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Spark supports loading files from the local filesystem, it requires that the files are available at the same path on all nodes in your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured data with Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is a component to work with structured and semistructured data. By structured data, we mean data that has a schema that is, a consistent set of fields across data records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive can store tables in a variety of formats, from plain text to column-oriented formats, inside HDFS or other storage systems. Spark SQL can load any table supported by Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load JSON data, first create a HiveContext as when using Hive. Then use the HiveContext.jsonFile method to get an RDD of Row objects for the whole file. Apart from using the whole Row object, you can also register this RDD as a table and select specific fields from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java Database Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can load data from any relational database that supports Java Database Con‐ nectivity (JDBC), including MySQL, Postgres, and other systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Cassandra connector is currently only available in Java and Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticsearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
